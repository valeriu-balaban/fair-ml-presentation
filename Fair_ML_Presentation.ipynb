{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fair-ML-Presentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8wqzdqJL9WPDfxfdIiuZi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeriu-balaban/fair-ml-presentation/blob/main/Fair_ML_Presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIUKw_LT2Mjg"
      },
      "source": [
        "# Learning Fair ML Models\n",
        "\n",
        " - Fairness will play a major role in automated systems such as ML\n",
        " - For now we do not always have many answers, just a lot of questions\n",
        " - Let's make it interactive. Think and answer questions \n",
        " - There is no right of wrong answer\n",
        " - Presented methods can be easily integrated in any project\n",
        " - Code is in vanila Pytorch and can be ported to Keras, Tensorflow, JAX, etc.\n",
        "\n",
        "**Outline:**\n",
        "1. When things go wrong, examples\n",
        "1. Datasets description, what a fair model should do.\n",
        "1. Subgroup, individual fairness\n",
        "1. Definition and fairness metrics\n",
        "1. Conditional Value at Risk method\n",
        "1. Wasserstein distance (maybe)\n",
        "1. Variance penalization and reference group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qecy1h8JqIb"
      },
      "source": [
        "## 1. Problem with $``the\\ algorithm\"$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hquMcFeXF0yT"
      },
      "source": [
        "\n",
        "  - Tiktok's algorithm prioritizes white people:\n",
        "    ![tiktok](https://raw.githubusercontent.com/valeriu-balaban/fair-ml-presentation/main/Screenshot%202021-07-19%20at%2011-31-32%20tiktok%20race%20problem%20at%20DuckDuckGo.png)\n",
        "\n",
        "  - Youtube's algorithm has other issues ([source](https://www.wsj.com/articles/youtubes-search-algorithm-directs-viewers-to-false-and-sexualized-videos-study-finds-11625644803)):\n",
        "  ![youtube](https://raw.githubusercontent.com/valeriu-balaban/fair-ml-presentation/main/Screenshot%202021-07-19%20at%2011-30-24%20YouTube%E2%80%99s%20Search%20Algorithm%20Directs%20Viewers%20to%20False%20and%20Sexualized%20Videos%2C%20Study%20Finds.png)\n",
        "\n",
        "  - Facebook's algorithm has ome other problems ([source](https://www.technologyreview.com/2019/04/05/1175/facebook-algorithm-discriminates-ai-bias/)):\n",
        "  ![facebook](https://raw.githubusercontent.com/valeriu-balaban/fair-ml-presentation/main/Screenshot%202021-07-19%20at%2011-35-14%20Facebook%E2%80%99s%20ad-serving%20algorithm%20discriminates%20by%20gender%20and%20race.png)\n",
        "\n",
        "  - And the same problem can be posed for self-driving cars ([source](https://www.technologyreview.com/2018/10/24/139313/a-global-ethics-study-aims-to-help-ai-solve-the-self-driving-trolley-problem/))\n",
        "  ![self-driving-cars](https://raw.githubusercontent.com/valeriu-balaban/fair-ml-presentation/main/Screenshot%202021-07-19%20at%2012-01-08%20Should%20a%20self-driving%20car%20kill%20the%20baby%20or%20the%20grandma%20Depends%20on%20where%20you%E2%80%99re%20from%20.png)\n",
        "\n",
        "  - Evaluating the danger of an offender during (pre)trial ([source](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm))\n",
        "    ![propublica](https://static.propublica.org/projects/algorithmic-bias/assets/img/generated/methodology-risk-of-recidivism-scores-by-race-900*363-482d1c.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8nMdiUKb9Hb"
      },
      "source": [
        "None of these companies designed the ML algorithm to be biased. Yet it ended up biased on multiple occasions. \n",
        "\n",
        "**Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0EEk_70cMTq"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n2aj4fFcO-E"
      },
      "source": [
        "*Learning from data without suppervision can (usually does) lead to issues because humans that generate the data discriminate by race, skin color, gender, age, etc.*\n",
        "\n",
        "https://www.youtube.com/embed/NaWJhlDb6sE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9K7SNumLJj3"
      },
      "source": [
        "## Definition and fainess metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAWOUZCkLPlY"
      },
      "source": [
        "### Notation:\n",
        "\n",
        "To simplify things we will focus only on classification. Although, the methods work for both, regression and classification.\n",
        "\n",
        "\n",
        "- $X$ - input ($x$ one input sample)\n",
        "- $Y$ - labels ($y$ one label sample)\n",
        "- $S$ - sensitive attributes which are part of X (e.g., gender, age, race)\n",
        "- $f$ - model we train, i.e., $f(x; \\theta)$ returns the probability of each labels\n",
        "- $\\theta$ - model parameters, e.g., the weights and the biases for NN\n",
        "- $\\ell$ - loss function, here we will always use cross-entropy ($-log f(x; \\theta) [y]$)\n",
        "- $\\mathbb{E}[\\ell(f, X, Y)]$ - loss value, i.e., average loss\n",
        "- $\\min\\mathbb{E}[\\ell(f, X, Y)]$ - optimization objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYegnLkBP7hY"
      },
      "source": [
        "### Definitions of perfect fairness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2VCxV6DVkma"
      },
      "source": [
        "\n",
        "- *Demografic parity:* $S \\perp f(X; \\theta)$\n",
        "- *Equaized odds:* $S \\perp f(X; \\theta) \\mid Y$\n",
        "- *Lack of disparate mistreatment:* $\\mathbb{P}(f(X; \\theta) \\neq Y \\mid S = s_a) = \\mathbb{P}(f(X; \\theta) \\neq Y \\mid S = s_b) , \\forall s_a, s_b \\in S$\n",
        "\n",
        "**Questions:**\n",
        "1. What is the difference between the first two from the perspective of application?\n",
        "1. What is the problem with these definitions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP5413u4VoIY"
      },
      "source": [
        "### Approximate fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ujakf4hVsp"
      },
      "source": [
        "- *Mean difference score:* $\\max \\mid \\mathbb{P}(f(X; \\theta) = y \\mid S = s_a) - \\mathbb{P}(f(X; \\theta) = y \\mid S = s_b) \\mid, \\forall s_a, s_b \\in S, \\forall y \\in Y$\n",
        "\n",
        "- *Covariance:* $\\mathbb{E}[f(X; \\theta) \\cdot S] - \\mathbb{E}[f(X; \\theta)] \\cdot \\mathbb{E}[S]$\n",
        "\n",
        "**Questions:**\n",
        "1. Why these two metrics are approximate\n",
        "1. Why we use the approximate metrics instead of perfect metrics of fairness?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBI5OiXWLOx-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}